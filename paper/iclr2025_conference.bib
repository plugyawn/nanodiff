@article{avsec2021effective,
  title={Effective gene expression prediction from sequence by integrating long-range interactions},
  author={Avsec, {\v{Z}}iga and Agarwal, Vikram and Visentin, Daniel and Ledsam, Joseph R and Grabska-Barwinska, Agnieszka and Taylor, Kyle R and Assael, Yannis and Jumper, John and Kohli, Pushmeet and Kelley, David R},
  journal={Nature methods},
  volume={18},
  number={10},
  pages={1196--1203},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}

@article{baevski2018adaptive,
  title={Adaptive input representations for neural language modeling},
  author={Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1809.10853},
  year={2018}
}

@article{campbell2022continuous,
  title={A continuous time framework for discrete denoising models},
  author={Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Thomas and Deligiannidis, George and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28266--28279},
  year={2022}
}

@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dong2024flex,
  title={Flex Attention: A Programming Model for Generating Optimized Attention Kernels},
  author={Dong, Juechu and Feng, Boyuan and Guessous, Driss and Liang, Yanbo and He, Horace},
  journal={arXiv preprint arXiv:2412.05496},
  year={2024}
}
@article{hg38,
  title={Genome reference consortium human build 37 (grch37},
  author={Genome Reference Consortium},
  journal={Database (GenBank or RefSeq)},
  year={2009}
}

@article{dhariwal2021agn,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{grevsova2023genomic,
  title={Genomic benchmarks: a collection of datasets for genomic sequence classification},
  author={Gre{\v{s}}ov{\'a}, Katar{\'\i}na and Martinek, Vlastimil and {\v{C}}ech{\'a}k, David and {\v{S}}ime{\v{c}}ek, Petr and Alexiou, Panagiotis},
  journal={BMC Genomic Data},
  volume={24},
  number={1},
  pages={25},
  year={2023},
  publisher={Springer}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{gulrajani2024plaid,
  title={Likelihood-based diffusion language models},
  author={Gulrajani, Ishaan and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{han2023ssd2,
  title={David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs},
  author={Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia and Ghazvininejad, Marjan},
  journal={arXiv preprint arXiv:2305.14771},
  year={2023}
}

@article{he2022diffusionbert,
  title={DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models},
  author={He, Zhengfu and Sun, Tianxiang and Wang, Kuanning and Huang, Xuanjing and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2211.15029},
  year={2022}
}


@article{hoogeboom2021argmax,
  title={Argmax flows and multinomial diffusion: Learning categorical distributions},
  author={Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12454--12465},
  year={2021}
}

@article{hoogeboom2021autoregressive,
  title={Autoregressive diffusion models},
  author={Hoogeboom, Emiel and Gritsenko, Alexey A and Bastings, Jasmijn and Poole, Ben and Berg, Rianne van den and Salimans, Tim},
  journal={arXiv preprint arXiv:2110.02037},
  year={2021}
}

@article{kingma2024understanding,
  title={Understanding diffusion objectives as the elbo with simple data augmentation},
  author={Kingma, Diederik and Gao, Ruiqi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kingma2021variational,
  title={Variational diffusion models},
  author={Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={21696--21707},
  year={2021}
}

@inproceedings{
    kou2024cllms,
    title={{CLLM}s: Consistency Large Language Models},
    author={Siqi Kou and Lanxiang Hu and Zhezhi He and Zhijie Deng and Hao Zhang},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=8uzBOVmh8H}
}

@inproceedings{
    lou2024discrete,
    title={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
    author={Aaron Lou and Chenlin Meng and Stefano Ermon},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=CNicRIVIPA}
}

@article{li2021discovering,
  title={Discovering non-monotonic autoregressive orderings with variational inference},
  author={Li, Xuanlin and Trabucco, Brandon and Park, Dong Huk and Luo, Michael and Shen, Sheng and Darrell, Trevor and Gao, Yang},
  journal={arXiv preprint arXiv:2110.15797},
  year={2021}
}

@article{li2022diffusion,
  title={Diffusion-lm improves controllable text generation},
  author={Li, Xiang and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4328--4343},
  year={2022}
}

@article{nguyen2024hyenadna,
  title={Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution},
  author={Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Wornow, Michael and Birch-Sykes, Callum and Massaroli, Stefano and Patel, Aman and Rabideau, Clayton and Bengio, Yoshua and others},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{
    ou2025your,
    title={Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data},
    author={Jingyang Ou and Shen Nie and Kaiwen Xue and Fengqi Zhu and Jiacheng Sun and Zhenguo Li and Chongxuan Li},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=sMyXP8Tanm}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@article{raffel2020t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{ren2019adaptive,
  title={Adaptive antithetic sampling for variance reduction},
  author={Ren, Hongyu and Zhao, Shengjia and Ermon, Stefano},
  booktitle={International Conference on Machine Learning},
  pages={5420--5428},
  year={2019},
  organization={PMLR}
}

@article{roeder2017sticking,
  title={Sticking the landing: Simple, lower-variance gradient estimators for variational inference},
  author={Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David K},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{
sahoo2023backpropagation,
title={Backpropagation through Combinatorial Algorithms: Identity with Projection Works},
author={Subham Sekhar Sahoo and Anselm Paulus and Marin Vlastelica and V{\'\i}t Musil and Volodymyr Kuleshov and Georg Martius},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=JZMR727O29}
}

@inproceedings{sahoo2024diffusion,
  title={Diffusion Models With Learned Adaptive Noise},
  author={Subham Sekhar Sahoo and Aaron Gokaslan and Christopher De Sa and Volodymyr Kuleshov},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=loMa99A4p8}}

@misc{
sahoo2024zeroorder,
title={Zero-Order Diffusion Guidance for Inverse Problems},
author={Subham Sekhar Sahoo and John Xavier Morris and Aaron Gokaslan and Srijeeta Biswas and Vitaly Shmatikov and Volodymyr Kuleshov},
year={2024},
url={https://openreview.net/forum?id=JBgBrnhLLL}
}

@article{shazeer2018mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{
    shi2024simplified,
    title={Simplified and Generalized Masked Diffusion for Discrete Data},
    author={Jiaxin Shi and Kehang Han and Zhe Wang and Arnaud Doucet and Michalis Titsias},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=xcqSOfHt4g}
}

@article{schiff2024caduceusICML,
  title={Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling},
  author={Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{schiff2024caduceus,
  title={Caduceus: Bi-directional equivariant long-range dna sequence modeling},
  author={Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2403.03234},
  year={2024}
}

@article{su2021roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@inproceedings{tay2021omninet,
  title={Omninet: Omnidirectional representations from transformers},
  author={Tay, Yi and Dehghani, Mostafa and Aribandi, Vamsi and Gupta, Jai and Pham, Philip M and Qin, Zhen and Bahri, Dara and Juan, Da-Cheng and Metzler, Donald},
  booktitle={International Conference on Machine Learning},
  pages={10193--10202},
  year={2021},
  organization={PMLR}
}

@article{wang2019bert,
  title={BERT has a mouth, and it must speak: BERT as a Markov random field language model},
  author={Wang, Alex and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1902.04094},
  year={2019}
}



@InProceedings{wang2023infodiff,
  title = 	 {{I}nfo{D}iffusion: Representation Learning Using Information Maximizing Diffusion Models},
  author =       {Wang, Yingheng and Schiff, Yair and Gokaslan, Aaron and Pan, Weishen and Wang, Fei and De Sa, Christopher and Kuleshov, Volodymyr},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {36336--36354},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/wang23ah/wang23ah.pdf},
  url = 	 {https://proceedings.mlr.press/v202/wang23ah.html},
  abstract = 	 {While diffusion models excel at generating high-quality samples, their latent variables typically lack semantic meaning and are not suitable for representation learning. Here, we propose InfoDiffusion, an algorithm that augments diffusion models with low-dimensional latent variables that capture high-level factors of variation in the data. InfoDiffusion relies on a learning objective regularized with the mutual information between observed and hidden variables, which improves latent space quality and prevents the latents from being ignored by expressive diffusion-based decoders. Empirically, we find that InfoDiffusion learns disentangled and human-interpretable latent representations that are competitive with state-of-the-art generative and contrastive methods, while retaining the high sample quality of diffusion models. Our method enables manipulating the attributes of generated images and has the potential to assist tasks that require exploring a learned latent space to generate quality samples, e.g., generative design.}
}

@inproceedings{uria2014deep,
  title={A deep and tractable density estimator},
  author={Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  booktitle={International Conference on Machine Learning},
  pages={467--475},
  year={2014},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chen2022analog,
  title={Analog bits: Generating discrete data using diffusion models with self-conditioning},
  author={Chen, Ting and Zhang, Ruixiang and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2208.04202},
  year={2022}
}

@article{han2022ssd,
  title={Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control},
  author={Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2210.17432},
  year={2022}
}

@article{strudel2022self,
  title={Self-conditioned embedding diffusion for text generation},
  author={Strudel, Robin and Tallec, Corentin and Altch{\'e}, Florent and Du, Yilun and Ganin, Yaroslav and Mensch, Arthur and Grathwohl, Will and Savinov, Nikolay and Dieleman, Sander and Sifre, Laurent and others},
  journal={arXiv preprint arXiv:2211.04236},
  year={2022}
}

@article{dieleman2022continuous,
  title={Continuous diffusion for categorical data},
  author={Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and others},
  journal={arXiv preprint arXiv:2211.15089},
  year={2022}
}

@article{lovelace2024latent,
  title={Latent diffusion for language generation},
  author={Lovelace, Justin and Kishore, Varsha and Wan, Chao and Shekhtman, Eliot and Weinberger, Kilian Q},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{sun2022score,
  title={Score-based continuous-time discrete diffusion models},
  author={Sun, Haoran and Yu, Lijun and Dai, Bo and Schuurmans, Dale and Dai, Hanjun},
  journal={arXiv preprint arXiv:2211.16750},
  year={2022}
}

@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}

@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chelba2014billion,
      title={One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling}, 
      author={Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
      year={2014},
      eprint={1312.3005},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{paperno-EtAl:2016:P16-1,
  author    = {Paperno, Denis  and  Kruszewski, Germ\'{a}n  and  Lazaridou,
Angeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,
Sandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},
  title     = {The {LAMBADA} dataset: Word prediction requiring a broad
discourse context},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {1525--1534},
  url       = {http://www.aclweb.org/anthology/P16-1144}
}

@inproceedings{Zhang2015CharacterlevelCN,
  title={Character-level Convolutional Networks for Text Classification},
  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  booktitle={NIPS},
  year={2015}
}

@article{Cohan_2018,
   title={A Discourse-Aware Attention Model for Abstractive Summarization of
            Long Documents},
   url={http://dx.doi.org/10.18653/v1/n18-2097},
   DOI={10.18653/v1/n18-2097},
   journal={Proceedings of the 2018 Conference of the North American Chapter of
          the Association for Computational Linguistics: Human Language
          Technologies, Volume 2 (Short Papers)},
   publisher={Association for Computational Linguistics},
   author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
   year={2018}
}

@misc{Gokaslan2019OpenWeb,
    title={OpenWebText Corpus},
    author={Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
    howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}},
    year={2019}
}

@article{mallet2021reverse,
  title={Reverse-complement equivariant networks for DNA sequences},
  author={Mallet, Vincent and Vert, Jean-Philippe},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13511--13523},
  year={2021}
}

@inproceedings{zhou2022towards,
  title={Towards a better understanding of reverse-complement equivariance for deep learning models in genomics},
  author={Zhou, Hannah and Shrikumar, Avanti and Kundaje, Anshul},
  booktitle={Machine Learning in Computational Biology},
  pages={1--33},
  year={2022},
  organization={PMLR}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{chang2022maskgit,
  title={Maskgit: Masked generative image transformer},
  author={Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11315--11325},
  year={2022}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@misc{portes2024mosaicbert,
      title={MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining}, 
      author={Jacob Portes and Alex Trott and Sam Havens and Daniel King and Abhinav Venigalla and Moin Nadeem and Nikhil Sardana and Daya Khudia and Jonathan Frankle},
      year={2024},
      eprint={2312.17482},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{israel2025enabling,
  title={Enabling Autoregressive Models to Fill In Masked Tokens},
  author={Israel, Daniel and Grover, Aditya and Broeck, Guy Van den},
  journal={arXiv preprint arXiv:2502.06901},
  year={2025}
}

@article{li2024derivative,
  title={Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding},
  author={Li, Xiner and Zhao, Yulai and Wang, Chenyu and Scalia, Gabriele and Eraslan, Gokcen and Nair, Surag and Biancalani, Tommaso and Ji, Shuiwang and Regev, Aviv and Levine, Sergey and others},
  journal={arXiv preprint arXiv:2408.08252},
  year={2024}
}

@article{goel2024memdlm,
  title={MeMDLM: De Novo Membrane Protein Design with Masked Discrete Diffusion Protein Language Models},
  author={Goel, Shrey and Thoutam, Vishrut and Marroquin, Edgar Mariano and Gokaslan, Aaron and Firouzbakht, Arash and Vincoff, Sophia and Kuleshov, Volodymyr and Kratochvil, Huong T and Chatterjee, Pranam},
  journal={arXiv preprint arXiv:2410.16735},
  year={2024}
}

@article{nisonoff2024unlocking,
  title={Unlocking guidance for discrete state-space diffusion and flow models},
  author={Nisonoff, Hunter and Xiong, Junhao and Allenspach, Stephan and Listgarten, Jennifer},
  journal={arXiv preprint arXiv:2406.01572},
  year={2024}
}

@inproceedings{
wang2018glue,
title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJ4km2R5t7},
}

@article{schiff2024discreteguidance,
  title={Simple Guidance Mechanisms for Discrete Diffusion Models},          
  author={Schiff, Yair and Sahoo, Subham Sekhar and Phung, Hao and Wang, Guanghan and Boshar, Sam and Dalla-torre, Hugo and de Almeida, Bernardo P and Rush, Alexander and Pierrot, Thomas and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2412.10193},
  year={2024}
}

@article{t5,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {transfer learning, natural language processing, multi-task learning, attention based models, deep learning}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@InProceedings{rasul2021timegrad,
  title = 	 {Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting},
  author =       {Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8857--8868},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rasul21a/rasul21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rasul21a.html},
  abstract = 	 {In this work, we propose TimeGrad, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.}
}


@misc{su2023roformer,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{pmlm,
    title = "Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order",
    author = "Liao, Yi  and
      Jiang, Xin  and
      Liu, Qun",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.24",
    doi = "10.18653/v1/2020.acl-main.24",
    pages = "263--274",
    abstract = "Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.",
}

@inproceedings{maskpredict,
    title = "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
    author = "Ghazvininejad, Marjan  and
      Levy, Omer  and
      Liu, Yinhan  and
      Zettlemoyer, Luke",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1633",
    doi = "10.18653/v1/D19-1633",
    pages = "6112--6121",
    abstract = "Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.",
}


@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{zhang2022styleswin,
  title={Styleswin: Transformer-based gan for high-resolution image generation},
  author={Zhang, Bowen and Gu, Shuyang and Zhang, Bo and Bao, Jianmin and Chen, Dong and Wen, Fang and Wang, Yong and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11304--11314},
  year={2022}
}


@misc{press,
      title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}, 
      author={Ofir Press and Noah A. Smith and Mike Lewis},
      year={2022},
      eprint={2108.12409},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vignac2022digress,
  title={Digress: Discrete denoising diffusion for graph generation},
  author={Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
  journal={arXiv preprint arXiv:2209.14734},
  year={2022}
}

@article{sun2023difusco,
  title={Difusco: Graph-based diffusion solvers for combinatorial optimization},
  author={Sun, Zhiqing and Yang, Yiming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={3706--3731},
  year={2023}
}

@inproceedings{avdeyev2023dirichlet,
  title={Dirichlet diffusion score model for biological sequence generation},
  author={Avdeyev, Pavel and Shi, Chenlai and Tan, Yuhao and Dudnyk, Kseniia and Zhou, Jian},
  booktitle={International Conference on Machine Learning},
  pages={1276--1301},
  year={2023},
  organization={PMLR}
}

@inproceedings{santilli2023accelerating,
    title = "Accelerating Transformer Inference for Translation via Parallel Decoding",
    author = "Santilli, Andrea  and
      Severino, Silvio  and
      Postolache, Emilian  and
      Maiorca, Valentino  and
      Mancusi, Michele  and
      Marin, Riccardo  and
      Rodola, Emanuele",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.689",
    doi = "10.18653/v1/2023.acl-long.689",
    pages = "12336--12355",
    abstract = "Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38{\%} w.r.t. the standard autoregressive decoding and nearly 2x when scaling the method on parallel resources. Finally, we introduce a decoding dependency graph visualizer (DDGviz) that let us see how the model has learned the conditional dependence between tokens and inspect the decoding procedure.",
}

@inproceedings{stern2019insertion,
  title={Insertion transformer: Flexible sequence generation via insertion operations},
  author={Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={5976--5985},
  year={2019},
  organization={PMLR}
}

@article{gu2019insertion,
  title={Insertion-based decoding with automatically inferred generation order},
  author={Gu, Jiatao and Liu, Qi and Cho, Kyunghyun},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={661--676},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{lee2018deterministic,
  title={Deterministic non-autoregressive neural sequence modeling by iterative refinement},
  author={Lee, Jason and Mansimov, Elman and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1802.06901},
  year={2018}
}

@inproceedings{
    sahoo2024simple,
    title={Simple and Effective Masked Diffusion Language Models},
    author={Subham Sekhar Sahoo and Marianne Arriola and Aaron Gokaslan and Edgar Mariano Marroquin and Alexander M Rush and Yair Schiff and Justin T Chiu and Volodymyr Kuleshov},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=L4uaAR4ArM}
}

@article{dunham2012encode,
   abstract = {The human genome encodes the blueprint of life, but the function of the vast majority of its nearly three billion bases is unknown. The Encyclopedia of DNA Elements (ENCODE) project has systematically mapped regions of transcription, transcription factor association, chromatin structure and histone modification. These data enabled us to assign biochemical functions for 80% of the genome, in particular outside of the well-studied protein-coding regions. Many discovered candidate regulatory elements are physically associated with one another and with expressed genes, providing new insights into the mechanisms of gene regulation. The newly identified elements also show a statistical correspondence to sequence variants linked to human disease, and can thereby guide interpretation of this variation. Overall, the project provides new insights into the organization and regulation of our genes and genome, and is an expansive resource of functional annotations for biomedical research. © 2012 Macmillan Publishers Limited. All rights reserved.},
   author = {Ian Dunham and Anshul Kundaje and Shelley F. Aldred and Patrick J. Collins and Carrie A. Davis and Francis Doyle and Charles B. Epstein and Seth Frietze and Jennifer Harrow and Rajinder Kaul and Jainab Khatun and Bryan R. Lajoie and Stephen G. Landt and Bum Kyu Lee and Florencia Pauli and Kate R. Rosenbloom and Peter Sabo and Alexias Safi and Amartya Sanyal and Noam Shoresh and Jeremy M. Simon and Lingyun Song and Nathan D. Trinklein and Robert C. Altshuler and Ewan Birney and James B. Brown and Chao Cheng and Sarah Djebali and Xianjun Dong and Jason Ernst and Terrence S. Furey and Mark Gerstein and Belinda Giardine and Melissa Greven and Ross C. Hardison and Robert S. Harris and Javier Herrero and Michael M. Hoffman and Sowmya Iyer and Manolis Kellis and Pouya Kheradpour and Timo Lassmann and Qunhua Li and Xinying Lin and Georgi K. Marinov and Angelika Merkel and Ali Mortazavi and Stephen C.J. Parker and Timothy E. Reddy and Joel Rozowsky and Felix Schlesinger and Robert E. Thurman and Jie Wang and Lucas D. Ward and Troy W. Whitfield and Steven P. Wilder and Weisheng Wu and Hualin S. Xi and Kevin Y. Yip and Jiali Zhuang and Bradley E. Bernstein and Eric D. Green and Chris Gunter and Michael Snyder and Michael J. Pazin and Rebecca F. Lowdon and Laura A.L. Dillon and Leslie B. Adams and Caroline J. Kelly and Julia Zhang and Judith R. Wexler and Peter J. Good and Elise A. Feingold and Gregory E. Crawford and Job Dekker and Laura Elnitski and Peggy J. Farnham and Morgan C. Giddings and Thomas R. Gingeras and Roderic Guigó and Timothy J. Hubbard and W. James Kent and Jason D. Lieb and Elliott H. Margulies and Richard M. Myers and John A. Stamatoyannopoulos and Scott A. Tenenbaum and Zhiping Weng and Kevin P. White and Barbara Wold and Yanbao Yu and John Wrobel and Brian A. Risk and Harsha P. Gunawardena and Heather C. Kuiper and Christopher W. Maier and Ling Xie and Xian Chen and Tarjei S. Mikkelsen and Shawn Gillespie and Alon Goren and Oren Ram and Xiaolan Zhang and Li Wang and Robbyn Issner and Michael J. Coyne and Timothy Durham and Manching Ku and Thanh Truong and Matthew L. Eaton and Alex Dobin and Andrea Tanzer and Julien Lagarde and Wei Lin and Chenghai Xue and Brian A. Williams and Chris Zaleski and Maik Röder and Felix Kokocinski and Rehab F. Abdelhamid and Tyler Alioto and Igor Antoshechkin and Michael T. Baer and Philippe Batut and Ian Bell and Kimberly Bell and Sudipto Chakrabortty and Jacqueline Chrast and Joao Curado and Thomas Derrien and Jorg Drenkow and Erica Dumais and Jackie Dumais and Radha Duttagupta and Megan Fastuca and Kata Fejes-Toth and Pedro Ferreira and Sylvain Foissac and Melissa J. Fullwood and Hui Gao and David Gonzalez and Assaf Gordon and Cédric Howald and Sonali Jha and Rory Johnson and Philipp Kapranov and Brandon King and Colin Kingswood and Guoliang Li and Oscar J. Luo and Eddie Park and Jonathan B. Preall and Kimberly Presaud and Paolo Ribeca and Daniel Robyr and Xiaoan Ruan and Michael Sammeth and Kuljeet Singh Sandhu and Lorain Schaeffer and Lei Hoon See and Atif Shahab and Jorgen Skancke and Ana Maria Suzuki and Hazuki Takahashi and Hagen Tilgner and Diane Trout and Nathalie Walters and Huaien Wang and Yoshihide Hayashizaki and Alexandre Reymond and Stylianos E. Antonarakis and Gregory J. Hannon and Yijun Ruan and Piero Carninci and Cricket A. Sloan and Katrina Learned and Venkat S. Malladi and Matthew C. Wong and Galt P. Barber and Melissa S. Cline and Timothy R. Dreszer and Steven G. Heitner and Donna Karolchik and Vanessa M. Kirkup and Laurence R. Meyer and Jeffrey C. Long and Morgan Maddren and Brian J. Raney and Linda L. Grasfeder and Paul G. Giresi and Anna Battenhouse and Nathan C. Sheffield and Kimberly A. Showers and Darin London and Akshay A. Bhinge and Christopher Shestak and Matthew R. Schaner and Seul Ki Kim and Zhuzhu Z. Zhang and Piotr A. Mieczkowski and Joanna O. Mieczkowska and Zheng Liu and Ryan M. McDaniell and Yunyun Ni and Naim U. Rashid and Min Jae Kim and Sheera Adar and Zhancheng Zhang and Tianyuan Wang and Deborah Winter and Damian Keefe and Vishwanath R. Iyer and Meizhen Zheng and Ping Wang and Jason Gertz and Jost Vielmetter and E. Christopher Partridge and Katherine E. Varley and Clarke Gasper and Anita Bansal and Shirley Pepke and Preti Jain and Henry Amrhein and Kevin M. Bowling and Michael Anaya and Marie K. Cross and Michael A. Muratet and Kimberly M. Newberry and Kenneth McCue and Amy S. Nesmith and Katherine I. Fisher-Aylor and Barbara Pusey and Gilberto DeSalvo and Stephanie L. Parker and Sreeram Balasubramanian and Nicholas S. Davis and Sarah K. Meadows and Tracy Eggleston and J. Scott Newberry and Shawn E. Levy and Devin M. Absher and Wing H. Wong and Matthew J. Blow and Axel Visel and Len A. Pennachio and Hanna M. Petrykowska and Alexej Abyzov and Bronwen Aken and Daniel Barrell and Gemma Barson and Andrew Berry and Alexandra Bignell and Veronika Boychenko and Giovanni Bussotti and Claire Davidson and Gloria Despacio-Reyes and Mark Diekhans and Iakes Ezkurdia and Adam Frankish and James Gilbert and Jose Manuel Gonzalez and Ed Griffiths and Rachel Harte and David A. Hendrix and Toby Hunt and Irwin Jungreis and Mike Kay and Ekta Khurana and Jing Leng and Michael F. Lin and Jane Loveland and Zhi Lu and Deepa Manthravadi and Marco Mariotti and Jonathan Mudge and Gaurab Mukherjee and Cedric Notredame and Baikang Pei and Jose Manuel Rodriguez and Gary Saunders and Andrea Sboner and Stephen Searle and Cristina Sisu and Catherine Snow and Charlie Steward and Electra Tapanari and Michael L. Tress and Marijke J. Van Baren and Stefan Washietl and Laurens Wilming and Amonida Zadissa and Zhengdong Zhang and Michael Brent and David Haussler and Alfonso Valencia and Nick Addleman and Roger P. Alexander and Raymond K. Auerbach and Suganthi Balasubramanian and Keith Bettinger and Nitin Bhardwaj and Alan P. Boyle and Alina R. Cao and Philip Cayting and Alexandra Charos and Yong Cheng and Catharine Eastman and Ghia Euskirchen and Joseph D. Fleming and Fabian Grubert and Lukas Habegger and Manoj Hariharan and Arif Harmanci and Sushma Iyengar and Victor X. Jin and Konrad J. Karczewski and Maya Kasowski and Phil Lacroute and Hugo Lam and Nathan Lamarre-Vincent and Jin Lian and Marianne Lindahl-Allen and Renqiang Min and Benoit Miotto and Hannah Monahan and Zarmik Moqtaderi and Xinmeng J. Mu and Henriette O'Geen and Zhengqing Ouyang and Dorrelyn Patacsil and Debasish Raha and Lucia Ramirez and Brian Reed and Minyi Shi and Teri Slifer and Heather Witt and Linfeng Wu and Xiaoqin Xu and Koon Kiu Yan and Xinqiong Yang and Kevin Struhl and Sherman M. Weissman and Luiz O. Penalva and Subhradip Karmakar and Raj R. Bhanvadia and Alina Choudhury and Marc Domanus and Lijia Ma and Jennifer Moran and Alec Victorsen and Thomas Auer and Lazaro Centanin and Michael Eichenlaub and Franziska Gruhl and Stephan Heermann and Burkhard Hoeckendorf and Daigo Inoue and Tanja Kellner and Stephan Kirchmaier and Claudia Mueller and Robert Reinhardt and Lea Schertel and Stephanie Schneider and Rebecca Sinn and Beate Wittbrodt and Jochen Wittbrodt and Gaurav Jain and Gayathri Balasundaram and Daniel L. Bates and Rachel Byron and Theresa K. Canfield and Morgan J. Diegel and Douglas Dunn and Abigail K. Ebersol and Tristan Frum and Kavita Garg and Erica Gist and R. Scott Hansen and Lisa Boatman and Eric Haugen and Richard Humbert and Audra K. Johnson and Ericka M. Johnson and Tattyana V. Kutyavin and Kristen Lee and Dimitra Lotakis and Matthew T. Maurano and Shane J. Neph and Fiedencio V. Neri and Eric D. Nguyen and Hongzhu Qu and Alex P. Reynolds and Vaughn Roach and Eric Rynes and Minerva E. Sanchez and Richard S. Sandstrom and Anthony O. Shafer and Andrew B. Stergachis and Sean Thomas and Benjamin Vernot and Jeff Vierstra and Shinny Vong and Hao Wang and Molly A. Weaver and Yongqi Yan and Miaohua Zhang and Joshua M. Akey and Michael Bender and Michael O. Dorschner and Mark Groudine and Michael J. MacCoss and Patrick Navas and George Stamatoyannopoulos and Kathryn Beal and Alvis Brazma and Paul Flicek and Nathan Johnson and Margus Lukk and Nicholas M. Luscombe and Daniel Sobral and Juan M. Vaquerizas and Serafim Batzoglou and Arend Sidow and Nadine Hussami and Sofia Kyriazopoulou-Panagiotopoulou and Max W. Libbrecht and Marc A. Schaub and Webb Miller and Peter J. Bickel and Balazs Banfai and Nathan P. Boley and Haiyan Huang and Jingyi Jessica Li and William Stafford Noble and Jeffrey A. Bilmes and Orion J. Buske and Avinash D. Sahu and Peter V. Kharchenko and Peter J. Park and Dannon Baker and James Taylor and Lucas Lochovsky},
   doi = {10.1038/nature11247},
   issn = {14764687},
   issue = {7414},
   journal = {Nature},
   title = {An integrated encyclopedia of DNA elements in the human genome},
   volume = {489},
   year = {2012},
}

@article{ho2022video,
title={Video diffusion models},
author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
journal={arXiv:2204.03458},
year={2022}}
}

@misc{yang2024videonewlanguagerealworld,
      title={Video as the New Language for Real-World Decision Making}, 
      author={Sherry Yang and Jacob Walker and Jack Parker-Holder and Yilun Du and Jake Bruce and Andre Barreto and Pieter Abbeel and Dale Schuurmans},
      year={2024},
      eprint={2402.17139},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.17139}, 
}

@misc{gupta2023photorealisticvideogenerationdiffusion,
      title={Photorealistic Video Generation with Diffusion Models}, 
      author={Agrim Gupta and Lijun Yu and Kihyuk Sohn and Xiuye Gu and Meera Hahn and Li Fei-Fei and Irfan Essa and Lu Jiang and Jose Lezama},
      year={2023},
      eprint={2312.06662},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.06662}, 
}

@misc{dhariwal2021diffusionmodelsbeatgans,
      title={Diffusion Models Beat GANs on Image Synthesis}, 
      author={Prafulla Dhariwal and Alex Nichol},
      year={2021},
      eprint={2105.05233},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.05233}, 
}

@inproceedings{
wu2023ardiffusion,
title={{AR}-Diffusion: Auto-Regressive Diffusion Model for Text Generation},
author={Tong Wu and Zhihao Fan and Xiao Liu and Hai-Tao Zheng and Yeyun Gong and yelong shen and Jian Jiao and Juntao Li and zhongyu wei and Jian Guo and Nan Duan and Weizhu Chen},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=0EG6qUQ4xE}
}

@article{zheng2024masked,
  title={Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling},
  author={Zheng, Kaiwen and Chen, Yongxin and Mao, Hanzi and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng},
  journal={arXiv preprint arXiv:2409.02908},
  year={2024}
}

@inproceedings{siautoregressive,
  title={Autoregressive Quantile Flows for Predictive Uncertainty Estimation},
  author={Si, Phillip and Bishop, Allan and Kuleshov, Volodymyr},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{si2023semi,
  title={Semi-autoregressive energy flows: exploring likelihood-free training of normalizing flows},
  author={Si, Phillip and Chen, Zeyi and Sahoo, Subham Sekhar and Schiff, Yair and Kuleshov, Volodymyr},
  booktitle={International Conference on Machine Learning},
  pages={31732--31753},
  year={2023},
  organization={PMLR}
}

@inproceedings{wang2023infodiffusion,
  title={Infodiffusion: Representation learning using information maximizing diffusion models},
  author={Wang, Yingheng and Schiff, Yair and Gokaslan, Aaron and Pan, Weishen and Wang, Fei and De Sa, Christopher and Kuleshov, Volodymyr},
  booktitle={International conference on machine learning},
  pages={36336--36354},
  year={2023},
  organization={PMLR}
}

@article{sahoo2022backpropagation,
  title={Backpropagation through combinatorial algorithms: Identity with projection works},
  author={Sahoo, Subham Sekhar and Paulus, Anselm and Vlastelica, Marin and Musil, V{\'\i}t and Kuleshov, Volodymyr and Martius, Georg},
  journal={arXiv preprint arXiv:2205.15213},
  year={2022}
}

@inproceedings{kuleshov2013fast,
  title={Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration},
  author={Kuleshov, Volodymyr},
  booktitle={International Conference on Machine Learning},
  pages={1418--1425},
  year={2013},
  organization={PMLR}
}

@inproceedings{gokaslan2024commoncanvas,
  title={Commoncanvas: Open diffusion models trained on creative-commons images},
  author={Gokaslan, Aaron and Cooper, A Feder and Collins, Jasmine and Seguin, Landan and Jacobson, Austin and Patel, Mihir and Frankle, Jonathan and Stephenson, Cory and Kuleshov, Volodymyr},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8250--8260},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{graves2023bayesian,
  title={Bayesian flow networks},
  author={Graves, Alex and Srivastava, Rupesh Kumar and Atkinson, Timothy and Gomez, Faustino},
  journal={arXiv preprint arXiv:2308.07037},
  year={2023}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@article{song2020denoising,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2010.02502},
  year={2020}
}

@article{chen2025diffusion,
  title={Diffusion forcing: Next-token prediction meets full-sequence diffusion},
  author={Chen, Boyuan and Mart{\'\i} Mons{\'o}, Diego and Du, Yilun and Simchowitz, Max and Tedrake, Russ and Sitzmann, Vincent},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={24081--24125},
  year={2025}
}

@article{hao2024training,
  title={Training large language models to reason in a continuous latent space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@article{kong2025scalable,
  title={Scalable Language Models with Posterior Inference of Latent Thought Vectors},
  author={Kong, Deqian and Zhao, Minglu and Xu, Dehong and Pang, Bo and Wang, Shu and Honig, Edouardo and Si, Zhangzhang and Li, Chuan and Xie, Jianwen and Xie, Sirui and others},
  journal={arXiv preprint arXiv:2502.01567},
  year={2025}
}

@article{gu2019levenshtein,
  title={Levenshtein transformer},
  author={Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{gloeckle2024better,
  title={Better \& faster large language models via multi-token prediction},
  author={Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\`e}re, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2404.19737},
  year={2024}
}

@article{gu2017non,
  title={Non-autoregressive neural machine translation},
  author={Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor OK and Socher, Richard},
  journal={arXiv preprint arXiv:1711.02281},
  year={2017}
}

@article{sanchez2023stay,
  title={Stay on topic with classifier-free guidance},
  author={Sanchez, Guillaume and Fan, Honglu and Spangher, Alexander and Levi, Elad and Ammanamanchi, Pawan Sasanka and Biderman, Stella},
  journal={arXiv preprint arXiv:2306.17806},
  year={2023}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}

@article{tian2025visual,
  title={Visual autoregressive modeling: Scalable image generation via next-scale prediction},
  author={Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  journal={Advances in neural information processing systems},
  volume={37},
  pages={84839--84865},
  year={2025}
}

@inproceedings{
zhao2024pard,
title={Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation},
author={Lingxiao Zhao and Xueying Ding and Leman Akoglu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=x4Kk4FxLs3}
}

@article{ye2024diffusion,
  title={Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models},
  author={Ye, Jiacheng and Gong, Shansan and Chen, Liheng and Zheng, Lin and Gao, Jiahui and Shi, Han and Wu, Chuan and Jiang, Xin and Li, Zhenguo and Bi, Wei and others},
  journal={arXiv preprint arXiv:2402.07754},
  year={2024}
}